{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Essential Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset, Subset\n",
    "\n",
    "\n",
    "from .model import PRC_Net\n",
    "from .Utils.load_train_data import *\n",
    "from .Utils.load_test_data import *\n",
    "from .Utils.loss import ccfl_dice\n",
    "from .Utils.training_utils import EarlyStopping, compute_iou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 'RGBNIRRE' # RGNIRRE (for Sequoia) | NGB (for Sesame Aerial)\n",
    "SEED = 0\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 320\n",
    "IMG_CHANNELS = 3\n",
    "N_CLASSES = 3\n",
    "batch_size = 16\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "AUGS = [1,2]\n",
    "early_stopping_patience = (EPOCHS // 5) * 2\n",
    "patience_lr = (EPOCHS // 5)\n",
    "\n",
    "\n",
    "filepath = \"PATH_TO_CHECKPOINT_DIR\"\n",
    "training_data_dir = \"PATH_TO_DATA_DIR\"\n",
    "training_masks_dir = \"PATH_TO_MASK_DIR\"\n",
    "testing_data_dir =  \"PATH_TO_DATA_DIR\"\n",
    "testing_masks_dir = \"PATH_TO_MASK_DIR\"\n",
    "OUT_DIR = \"PATH_for_Model_Trainig\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Load Training data for WeedMap (RedEdge & Sequoia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.mask_files = sorted(os.listdir(mask_dir))\n",
    "\n",
    "        # Each image will have 3 variants: original, h-flip, v-flip\n",
    "        self.total_variants = 3\n",
    "        self.total_images = len(self.image_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        # total = original + horizontal + vertical\n",
    "        return self.total_images * self.total_variants\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which image and which variant to load\n",
    "        img_idx = idx // self.total_variants\n",
    "        variant = idx % self.total_variants  # 0=original, 1=horizontal, 2=vertical\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[img_idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[img_idx])\n",
    "\n",
    "        # ----- Load image (.npy, 5-channel case) -----\n",
    "        image = np.load(image_path, allow_pickle=True).astype(np.float32)\n",
    "        image = image / 127.5 - 1.0  # normalize to [-1, 1]\n",
    "\n",
    "        # ----- Load mask -----\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.uint8)\n",
    "\n",
    "        # ----- Apply flipping augmentation -----\n",
    "        if variant == 1:  # horizontal flip\n",
    "            image = np.flip(image, axis=1).copy()\n",
    "            mask = np.flip(mask, axis=1).copy()\n",
    "        elif variant == 2:  # vertical flip\n",
    "            image = np.flip(image, axis=0).copy()\n",
    "            mask = np.flip(mask, axis=0).copy()\n",
    "\n",
    "        # ----- Apply Albumentations transform if available -----\n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]  # tensor (C, H, W)\n",
    "            mask = augmented[\"mask\"]    # tensor (H, W)\n",
    "        else:\n",
    "            image = np.transpose(image, (2, 0, 1))  # C, H, W\n",
    "            image = torch.from_numpy(image).float()\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        # ----- One-hot encode mask -----\n",
    "        mask_onehot = F.one_hot(mask.long(), num_classes=3).permute(2, 0, 1).float()\n",
    "\n",
    "        return image, mask, mask_onehot\n",
    "\n",
    "\n",
    "# ------------- Data Augmentations -------------\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "# Paths\n",
    "image_dir = training_data_dir\n",
    "mask_dir  = training_masks_dir\n",
    "\n",
    "# Collect all images (assumes names match between image_dir and mask_dir)\n",
    "all_images = sorted(os.listdir(image_dir))\n",
    "all_masks  = sorted(os.listdir(mask_dir))\n",
    "\n",
    "# Ensure consistency\n",
    "assert len(all_images) == len(all_masks), \"Mismatch between images and masks\"\n",
    "total = len(all_images)\n",
    "print(f\"Total base samples: {total}\")\n",
    "\n",
    "# Shuffle indices (based only on original images)\n",
    "indices = list(range(total))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split sizes (e.g., 80% train, 20% validation)\n",
    "val_size = int(0.2 * total)\n",
    "train_size = total - val_size\n",
    "\n",
    "# Assign indices for original (unaugmented) data\n",
    "val_indices  = indices[:val_size]\n",
    "train_indices = indices[val_size:]\n",
    "\n",
    "print(f\"Train: {len(train_indices)}, Val: {len(val_indices)}\")\n",
    "\n",
    "# ---------------- Dataset Construction ----------------\n",
    "# Full dataset (with flipping augmentation)\n",
    "full_dataset = SegmentationDataset(image_dir, mask_dir, transform=None)\n",
    "\n",
    "# Training subset (augmented: original + H-flip + V-flip)\n",
    "train_dataset = Subset(full_dataset, [i * 3 + j for i in train_indices for j in range(3)])\n",
    "\n",
    "# Validation subset (originals only)\n",
    "val_dataset = Subset(full_dataset, [i * 3 for i in val_indices])\n",
    "\n",
    "# Apply transforms\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform   = val_transform\n",
    "\n",
    "# ---------------- DataLoaders ----------------\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Final dataset sizes → Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Load Training data for Sesame Aerial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.mask_files = sorted(os.listdir(mask_dir))\n",
    "\n",
    "        # Each image will have 3 variants: original, h-flip, v-flip\n",
    "        self.total_variants = 3\n",
    "        self.total_images = len(self.image_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        # total = original + horizontal + vertical\n",
    "        return self.total_images * self.total_variants\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which image and which variant to load\n",
    "        img_idx = idx // self.total_variants\n",
    "        variant = idx % self.total_variants  # 0=original, 1=horizontal, 2=vertical\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[img_idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[img_idx])\n",
    "\n",
    "        # ----- Load image -----\n",
    "        image = np.array(Image.open(image_path)).astype(np.float32)\n",
    "        image = image / 127.5 - 1.0  # normalize to [-1, 1]\n",
    "\n",
    "        # ----- Load mask -----\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.uint8)\n",
    "\n",
    "        # ----- Apply flipping augmentation -----\n",
    "        if variant == 1:  # horizontal flip\n",
    "            image = np.flip(image, axis=1).copy()\n",
    "            mask = np.flip(mask, axis=1).copy()\n",
    "        elif variant == 2:  # vertical flip\n",
    "            image = np.flip(image, axis=0).copy()\n",
    "            mask = np.flip(mask, axis=0).copy()\n",
    "\n",
    "        # ----- Apply Albumentations transform if available -----\n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]  # tensor (C, H, W)\n",
    "            mask = augmented[\"mask\"]    # tensor (H, W)\n",
    "        else:\n",
    "            image = np.transpose(image, (2, 0, 1))  # C, H, W\n",
    "            image = torch.from_numpy(image).float()\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        # ----- One-hot encode mask -----\n",
    "        mask_onehot = F.one_hot(mask.long(), num_classes=3).permute(2, 0, 1).float()\n",
    "\n",
    "        return image, mask, mask_onehot\n",
    "\n",
    "\n",
    "# ------------- Data Augmentations -------------\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "# Paths\n",
    "image_dir = training_data_dir\n",
    "mask_dir  = training_masks_dir\n",
    "\n",
    "# Collect all images (assumes names match between image_dir and mask_dir)\n",
    "all_images = sorted(os.listdir(image_dir))\n",
    "all_masks  = sorted(os.listdir(mask_dir))\n",
    "\n",
    "# Ensure consistency\n",
    "assert len(all_images) == len(all_masks), \"Mismatch between images and masks\"\n",
    "total = len(all_images)\n",
    "print(f\"Total base samples: {total}\")\n",
    "\n",
    "# Shuffle indices (based only on original images)\n",
    "indices = list(range(total))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split sizes (e.g., 80% train, 20% validation)\n",
    "val_size = int(0.2 * total)\n",
    "train_size = total - val_size\n",
    "\n",
    "# Assign indices for original (unaugmented) data\n",
    "val_indices  = indices[:val_size]\n",
    "train_indices = indices[val_size:]\n",
    "\n",
    "print(f\"Train: {len(train_indices)}, Val: {len(val_indices)}\")\n",
    "\n",
    "# ---------------- Dataset Construction ----------------\n",
    "# Full dataset (with flipping augmentation)\n",
    "full_dataset = SegmentationDataset(image_dir, mask_dir, transform=None)\n",
    "\n",
    "# Training subset (augmented: original + H-flip + V-flip)\n",
    "train_dataset = Subset(full_dataset, [i * 3 + j for i in train_indices for j in range(3)])\n",
    "\n",
    "# Validation subset (originals only)\n",
    "val_dataset = Subset(full_dataset, [i * 3 for i in val_indices])\n",
    "\n",
    "# Apply transforms\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform   = val_transform\n",
    "\n",
    "# ---------------- DataLoaders ----------------\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Final dataset sizes → Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loss_fn(y_true_onehot, logits):\n",
    "    focal_dice = ccfl_dice(y_true_onehot, logits, lamda=0.7, from_logits=False)\n",
    "    return focal_dice.mean()\n",
    "\n",
    "# ------------- Model, Optimizer, Loss ------------- # commented for fine tuning\n",
    "model = PRC_Net(n_classes=N_CLASSES, IMG_HEIGHT=image_size, IMG_WIDTH=image_size, IMG_CHANNELS=IMG_CHANNELS, dropout_rate=0.0)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = loss_fn\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=patience_lr)\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_iou': [],\n",
    "    'val_iou': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Training Loop -------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_val_loss = float('inf') # Intialize best loss\n",
    "early_stopper = EarlyStopping(patience=early_stopping_patience, min_delta=0.001) # Early stopping\n",
    "early_stopping_counter = 0\n",
    "accumulation_steps = 1 \n",
    "start_training_time = datetime.datetime.now()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss, running_iou = 0, 0\n",
    "\n",
    "    for step, (images, masks, mask_onehot) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\")):\n",
    "        # images, masks, mask_onehot = images.to(device), masks.to(device)\n",
    "        images, masks, mask_onehot = images.to(device), masks.to(device), mask_onehot.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(mask_onehot, outputs)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient accumulation logic\n",
    "        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        running_iou += compute_iou(preds.cpu(), masks.cpu())\n",
    "\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_iou = running_iou / len(train_loader)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_iou'].append(train_iou)\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valr_loss, valr_iou = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks, mask_onehot in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "            images, masks, mask_onehot = images.to(device), masks.to(device), mask_onehot.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(mask_onehot, outputs)\n",
    "            valr_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            valr_iou += compute_iou(preds.cpu(), masks.cpu())\n",
    "    \n",
    "    val_loss = valr_loss / len(val_loader)\n",
    "    val_iou = valr_iou / len(val_loader)\n",
    "\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_iou'].append(val_iou)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Train IoU: {train_iou:.4f}, Val IoU: {val_iou:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_model_path = f\"{OUT_DIR}/PRC_NET_best_E-{epoch+1}.pth\"\n",
    "        print(f\"Validation loss improved from {best_val_loss} to {val_loss}, saving best model at {best_model_path}\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Best Model Saved for Epoch {epoch+1}. And reset early stopping counter {early_stopping_counter} to 0\")\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping has been triggered\")\n",
    "            break\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    early_stopper.step(val_loss)\n",
    "    \n",
    "    if early_stopper.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "end_training_time = datetime.datetime.now()\n",
    "\n",
    "total_training_time = end_training_time - start_training_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Model and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{OUT_DIR}/PRC_NET_last.pth\")\n",
    "with open(f\"{OUT_DIR}/history.pkl\", 'wb') as file:\n",
    "    pickle.dump(history, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Load Testing data for WeedMap (RedEdge & Sequoia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 3: dataset wrapper (expects numpy arrays from your Utils loader)\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.mask_files = sorted(os.listdir(mask_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "        print(image_path, mask_path)\n",
    "        print(f\"getting image\")\n",
    "\n",
    "        # ----- Load image (.npy, 5-channel case for WeedMap) -----\n",
    "        image = np.load(image_path, allow_pickle=True).astype(np.float32)\n",
    "        image = image / 127.5 - 1.0  # normalize to [-1, 1]\n",
    "\n",
    "        # ----- Load mask -----\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.uint8)\n",
    "\n",
    "        # ----- Apply Albumentations transform if available -----\n",
    "        if self.transform is not None:\n",
    "            # Albumentations expects HWC, not CHW\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]  # tensor, shape (C, H, W)\n",
    "            mask = augmented[\"mask\"]    # tensor, shape (H, W)\n",
    "        else:\n",
    "            # If no transform, manually convert\n",
    "            image = np.transpose(image, (2, 0, 1))  # C, H, W\n",
    "            image = torch.from_numpy(image).float()\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        # ----- Optional one-hot encoding -----\n",
    "        mask_onehot = F.one_hot(mask.long(), num_classes=3).permute(2, 0, 1).float()\n",
    "\n",
    "        return image, mask, mask_onehot\n",
    "\n",
    "\n",
    "\n",
    "# ------------- Data Augmentations -------------\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "test_image_dir = testing_data_dir\n",
    "test_mask_dir = testing_masks_dir\n",
    "\n",
    "\n",
    "# image_size = image_size\n",
    "image_size = 320\n",
    "batch_size = 1\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "\n",
    "# Full dataset\n",
    "test_dataset = SegmentationDataset(test_image_dir, test_mask_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Load Testing data for Sesame Aerial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 3: dataset wrapper (expects numpy arrays from your Utils loader)\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.mask_files = sorted(os.listdir(mask_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "        print(image_path, mask_path)\n",
    "        print(f\"getting image\")\n",
    "\n",
    "        image = np.array(Image.open(image_path)).astype(np.float32)\n",
    "        image = image / 127.5 - 1.0  # normalize to [-1, 1]\n",
    "\n",
    "        # ----- Load mask -----\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.uint8)\n",
    "\n",
    "        # ----- Apply Albumentations transform if available -----\n",
    "        if self.transform is not None:\n",
    "            # Albumentations expects HWC, not CHW\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]  # tensor, shape (C, H, W)\n",
    "            mask = augmented[\"mask\"]    # tensor, shape (H, W)\n",
    "        else:\n",
    "            # If no transform, manually convert\n",
    "            image = np.transpose(image, (2, 0, 1))  # C, H, W\n",
    "            image = torch.from_numpy(image).float()\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        # ----- Optional one-hot encoding -----\n",
    "        mask_onehot = F.one_hot(mask.long(), num_classes=3).permute(2, 0, 1).float()\n",
    "\n",
    "        return image, mask, mask_onehot\n",
    "\n",
    "\n",
    "\n",
    "# ------------- Data Augmentations -------------\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "\n",
    "# First, combine images and masks from all compaigns to one IMAGE_DIR and MASKS_DIR respectively. Then use the following code.\n",
    "test_image_dir = testing_data_dir\n",
    "test_mask_dir = testing_masks_dir\n",
    "\n",
    "\n",
    "# image_size = image_size\n",
    "image_size = 320\n",
    "batch_size = 1\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=image_size, width=image_size),\n",
    "    ToTensorV2(transpose_mask=True),\n",
    "])\n",
    "\n",
    "\n",
    "# Full dataset\n",
    "test_dataset = SegmentationDataset(test_image_dir, test_mask_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, masks, mask_onehot in tqdm(dataloader):\n",
    "            images = images.to(device) \n",
    "            masks = masks.to(device) \n",
    "            mask_onehot = mask_onehot.to(device)\n",
    "            probs = model(images)  # (B, 3, H, W)\n",
    "            preds = torch.argmax(probs, dim=1)  # (B, H, W)\n",
    "            all_preds.append(preds.cpu().view(-1))\n",
    "            all_labels.append(masks.cpu().view(-1))\n",
    "        y_pred = torch.cat(all_preds)\n",
    "        y_true = torch.cat(all_labels)\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Load Model and Run --------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PRC_Net(n_classes=N_CLASSES, IMG_HEIGHT=image_size, IMG_WIDTH=image_size, IMG_CHANNELS=IMG_CHANNELS, dropout_rate=0.0)\n",
    "model.load_state_dict(torch.load(\"model_path\", map_location=device))\n",
    "model = model.to(device) \n",
    "\n",
    "y_pred, y_true = perform_inference(model, test_loader, device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
